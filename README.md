# GPU_SENTIMENT_MODEL_TRAIN


2403110280 인공지능소프트웨어 이민수

-------------------------------------------------------------------------------------------------

1. 데이터 수집
  - GPU 게시판 : 쿨&조이.(Selenium)
  - ![image](https://github.com/user-attachments/assets/635e4e3f-c52e-4b13-88d9-4c4c27ba0d64)
  - 수집 데이터셋 : 63만개.

-------------------------------------------------------------------------------------------------

2. 데이터 라벨링
  - gemma3 사용해 긍정/중립/부정 라벨링.

-------------------------------------------------------------------------------------------------

4. 데이터 전처리
   - CSV 필터링 : 긍정/중립/부정 만 추출 후 불필요 라벨 제거.

   - 레이블 인코딩 : 문자열 라벨을 정수로 매핑
     - (긍정, 중립, 부정) 0, 1, 2.
   
   - 텍스트 정제 : 정규화를 사용해 한글/숫자/공백/!,? 외 모든 문자 삭제.

   - 형태소 토큰화 : KoNLPy Okt 분석기로 어간(원형)까지 분리.
     - 다양한 형태의 단어를 동일한 의미로 인식하기 위함

   - 불용어 제거 : korean_stopwords.txt(사용자 제작) 읽어 불용어 제거.

   - 문장 길이 통계 : 95%의 문장을 포함하는 길이를 max_len으로 설정해 과도한 패딩 방지.

   - 패딩 : 모든 샘플을 동일 길이 맞춰 일관된 입력 형태로 변형.

-------------------------------------------------------------------------------------------------

5. 모델 학습
   - 데이터 중 절반을 학습에 사용 후 나머지는 테스트 데이터로 사용.
   
   - 데이터 분할 : train/validation/test -> 8:1:1
   
   - Word2Vec 300 d 학습 -> 임베딩 매트릭스 생성
   
   - BI-LSTM(128 -> 64) + Attention + Dense → Softmax
     - BI-LSTM : 문장의 복잡한 특징을 폭 넓게 파악한 후(128) 더 적은 유닛(64)으로 특징만 남기기 위함
     - Attention : 문장에서 어떤 단어/부분 이 감성분석에서 가장 중요한지 찾아 해당 부분에 더 많은 가중치를 주기 위함
     - softmax : 정확률을 표시하기 위함

   - 워밍업 & 본 학습 : 3 epoch 워밍업 후 EarlyStopping+Checkpoint 로 100 epoch 탐색
     - 워밍업 : 모델이 초기에 불안정한 가중치로 인해 학습이 잘 진행되지 않는 것을 방지
   
   - 하이퍼파라미터 : batch 4096, dropout 0.5, L2 1e-4
     - L2, 1e-4 가중치를 규제해 모델의 복잡도를 낮추고 일반화 성능을 향상하기 위함
   
   - 모델 저장 : final_bilstm_attention.h5 저장.

-------------------------------------------------------------------------------------------------

6. 모델 정확도

| Metric                | Train | Validation | Test |
|-----------------------|:-----:|:----------:|:----:|
| Accuracy              | 0.74  | 0.69       | 0.66 |
| Macro Precision       | 0.75  | 0.70       | 0.66 |
| Macro Recall          | 0.74  | 0.69       | 0.66 |
| Macro F1-score        | 0.74  | 0.69       | 0.65 |
| Weighted Precision    | 0.75  | 0.69       | 0.68 |
| Weighted Recall       | 0.74  | 0.69       | 0.66 |
| Weighted F1-score     | 0.74  | 0.69       | 0.66 |

| Set         | Label | Precision | Recall | F1-score | Support |
|-------------|-------|:---------:|:------:|:--------:|--------:|
| **Train**   | 부정  | 0.76 | 0.79 | 0.77 | 146 185 |
|             | 중립  | 0.68 | 0.75 | 0.72 | 154 753 |
|             | 긍정  | 0.81 | 0.68 | 0.74 | 133 482 |
| **Validation** | 부정  | 0.71 | 0.73 | 0.72 | 16 243 |
|             | 중립  | 0.63 | 0.69 | 0.66 | 17 195 |
|             | 긍정  | 0.76 | 0.64 | 0.69 | 14 831 |
| **Test**    | 부정  | 0.76 | 0.71 | 0.73 | 131 493 |
|             | 중립  | 0.52 | 0.69 | 0.59 |  86 397 |
|             | 긍정  | 0.72 | 0.57 | 0.64 |  99 799 |




   - 주요 원인
     - 클래스 불균형으로 인해 부정 클래스에 편향됨
     - 자동 라벨링으로 인한 노이즈가 중립 라벨에 집중되어 중립 라벨에 오류가 몰려 Precision이 떨어짐.
     - 텍스트 전처리 중 도메인에 관련된 텍스트가 손실됨.
     - 학습 정확도 대비 테스트 정확도 간 차이가 있어 데이터가 편향과 노이즈에 과적합 됨.
  - 향후 개선점
    - 하나의 모델을 사용하여 텍스트 라벨링을 하는 것이 아닌 여러개의 모델을 사용해 텍스트 라벨링 진행
    - 도메인 전용 사전을 만들어 해당 도메인 용어를 제외하고 텍스트 전처리 진행
    - 정확도가 낮은 중립과 긍정 데이터가 대한 데이터 증강 진행
    - BERT, RoBERTa, ETRI, KoELECTRA 등 사전 학습 모델로 재학습 진행
